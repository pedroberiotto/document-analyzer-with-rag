# ğŸ“„ Document Analyser RAG

> A production-ready AI pipeline for extracting structured data from PDFs using RAG with OpenAI, LangChain, and Chroma.

![App](https://img.shields.io/badge/App-Streamlit-ff4b4b)
![Backend](https://img.shields.io/badge/API-FastAPI-009688)
![LLM](https://img.shields.io/badge/LLM-OpenAI-412991)
![RAG](https://img.shields.io/badge/Pattern-RAG-1f6feb)
![Vector%20Store](https://img.shields.io/badge/Vector%20Store-Chroma-00c853)
![Orchestration](https://img.shields.io/badge/Orchestration-LangChain-1a73e8)
![License](https://img.shields.io/badge/License-MIT-orange)

---

## ğŸ§  RAG Architecture

High-level flow from PDF upload to structured output:

```mermaid
flowchart LR
    A[Upload PDF\n(Streamlit / API)] --> B[PDF Loader\n(PyPDFLoader)]
    B --> C[Text Splitter\n(RecursiveCharacterTextSplitter)]
    C --> D[Embeddings\n(OpenAI text-embedding-3-small)]
    D --> E[Vector Store\n(Chroma per document_id)]

    subgraph RAG Loop per Field
        F[Extraction Schema\n(JSON: fields + descriptions)]
        F --> G[Build Field Query]
        G --> H[Retriever\n(E.from Chroma)]
        H --> I[LLM + Prompt\n(ChatOpenAI gpt-4.1-mini)]
        I --> J[FieldAnswer\n(value, confidence, justification)]
    end

    E --> H
    J --> K[Aggregation\nExtractionResult]
    K --> L[UI Response\n(values + confidence + evidence)]
```

---

## âœ¨ Features

- ğŸ”Œ **Bring-your-own schema**  
  Define custom fields in JSON (name + description + type).

- ğŸ“š **RAG over a single document**  
  Chunks the PDF, embeds with OpenAI, and uses a vector store (Chroma) to find the most relevant parts.

- ğŸ§  **LLM-powered field extraction**  
  Uses `gpt-4.1-mini` (via LangChain + OpenAI) with structured outputs (Pydantic).

- ğŸ” **Traceable answers**  
  Every field comes with:
  - `confidence` (0â€“1)
  - `sources[]` (page + text snippet used as evidence)

- ğŸ–¥ï¸ **Streamlit UI**  
  Simple interface to test schemas and documents without writing code.

- ğŸ§ª **API-ready**  
  FastAPI backend exposes `/documents/upload`, `/schemas`, `/extract` for programmatic use.

---

## ğŸ“‚ Project Structure

```bash
document-analyser-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # FastAPI app (optional, for REST API)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extraction_result.py # Output models (ExtractionResult, FieldResult, SourceSpan)
â”‚   â”‚   â””â”€â”€ extraction_schema.py # Input models (ExtractionSchema, ExtractionField)
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ ingestion_langchain.py  # PDF loading, splitting, embeddings, Chroma retriever
â”‚       â””â”€â”€ rag_langchain.py        # Field-by-field RAG pipeline
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/                 # Uploaded PDFs (created at runtime)
â”‚   â””â”€â”€ chroma/                  # Chroma persistence per document_id (created at runtime)
â”œâ”€â”€ streamlit_app.py             # Streamlit UI entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
